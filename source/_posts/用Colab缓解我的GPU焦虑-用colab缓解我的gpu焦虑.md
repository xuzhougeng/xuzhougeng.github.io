---
title: 用Colab缓解我的GPU焦虑
date: 2023-06-16 06:32:07.246
updated: 2023-06-16 06:32:07.246
url: /archives/用colab缓解我的gpu焦虑
categories: 
tags: 
---

大型语言模型（LLM）非常的火热，我就想着能不能自己在本地上部署一个开源的模型，比如说chatGLM-6B。尽管苹果公司自己的芯片支持pytorch的图形加速运算，但是真实体验下来，也只能说有待提高。

另一个方法就是配一个4090显卡的主机了，毕竟4090有24Gb的显存，跑个chatGLM-6B还是绰绰有余的。奈何，我多看了一眼LLaMA-60B，光模型文件都差不多80G了，总不可能去买一张A100 80G吧。

思来想去，还是听别人劝，用Google Colab吧。我测试了


那么问题来了，每次都要重新开资源，岂不是要花很多时间在环境配置和数据下载上？

事实上，你从Colab获取HuggingFace的模型时，速度是400Mb/s。